{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f85a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement TensorFlow==1.12.0\n",
      "ERROR: No matching distribution found for TensorFlow==1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install TensorFlow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4da75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn import \\\n",
    "    metrics\n",
    "from sklearn import svm, tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import \\\n",
    "    train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import \\\n",
    "    DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import shap\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8d321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.3.0\n",
      "numpy: 1.20.1\n",
      "sys: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
      "scipy: 1.6.2\n",
      "pandas: 1.2.4\n",
      "sklearn: 0.24.1\n",
      "seaborn: 0.11.1\n",
      "shap: 0.42.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd52cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_binary(x_input, args):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=128, strides=64, activation='relu', padding='same', input_shape=(x_input.shape[1], 1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization(name='bn1'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=128, strides=2, activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization(name='bn2'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    num_features = model.output_shape[1:]\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization(name='bn3'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hac(cor):\n",
    "\n",
    "    def mydist(p1, p2):\n",
    "        x = int(p1)\n",
    "        y = int(p2)\n",
    "        if cor[x, y] > 1.0:\n",
    "            return cor[x, y] - 1.0\n",
    "        return 1.0 - cor[x, y]\n",
    "\n",
    "    x = list(range(cor.shape[0]))\n",
    "    X = np.array(x)\n",
    "    \n",
    "\n",
    "    linked = linkage(np.reshape(X, (len(X), 1)), metric=mydist, method='single')\n",
    "    n = len(linked) + 1\n",
    "    cache = dict()\n",
    "    for k in range(len(linked)):\n",
    "        c1, c2 = int(linked[k][0]), int(linked[k][1])\n",
    "        c1 = [c1] if c1 < n else cache.pop(c1)\n",
    "        c2 = [c2] if c2 < n else cache.pop(c2)\n",
    "        cache[n+k] = c1 + c2\n",
    "    ix = cache[2*len(linked)]\n",
    " \n",
    "    \n",
    "    return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7714dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "def new_conv1d_layer(input, filters, kernel_size, strides, keep_prob, name, activation, padding):\n",
    "    with tf.init_scope():\n",
    "        layer = tf.keras.layers.Conv1D(filters, kernel_size, strides=strides, padding=padding)(input)\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            layer = tf.keras.activations.tanh(layer)\n",
    "        elif activation == 'relu':\n",
    "            layer = tf.keras.activations.relu(layer)\n",
    "\n",
    "        layer = tf.keras.layers.Dropout(rate=0)(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "def new_activation_layer(input, name):\n",
    "    with tf.init_scope():\n",
    "        if name == \"tanh\":\n",
    "            layer = tf.keras.activations.tanh(input)\n",
    "        elif name == \"relu\":\n",
    "            layer = tf.keras.activations.relu(input)\n",
    "\n",
    "\n",
    "        return layers\n",
    "    \n",
    "\n",
    "\n",
    "def new_fc_layer(input, num_inputs, num_outputs, keep_prob, name):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        weights = tf.random.truncated_normal([num_inputs, num_outputs], stddev=0.1)\n",
    "        biases = tf.constant(0.1, shape=[num_outputs])\n",
    "  \n",
    "        dropout = tf.nn.dropout(input, rate=0.2)\n",
    "        layer = tf.matmul(dropout, weights) + biases\n",
    "   \n",
    "\n",
    "        return layer \n",
    "\n",
    "\n",
    "\n",
    "def network_continous(x_input, keep_prob, args):\n",
    "    num_filter = args.kernel_size\n",
    "    window_size = args.window_size\n",
    "    stride_size =  args.strides\n",
    "\n",
    "    layer = new_conv1d_layer(input=x_input, filters=num_filter[0], kernel_size=window_size[0], strides=stride_size[0], keep_prob=keep_prob,name =\"conv1\",activation='tanh', padding='valid')\n",
    "\n",
    "    layer = new_conv1d_layer(input=layer, filters=num_filter[1], kernel_size=window_size[1], strides=stride_size[1], keep_prob=keep_prob,name =\"conv2\",activation='tanh',padding='valid')\n",
    "\n",
    "    layer = new_conv1d_layer(input=layer, filters=num_filter[2], kernel_size=window_size[2], strides=stride_size[2], keep_prob=keep_prob,name =\"conv3\",activation='tanh',padding='valid')\n",
    "\n",
    "    num_features = layer.get_shape()[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "\n",
    "    layer = new_fc_layer(input=layer, num_inputs=num_features,keep_prob=keep_prob, num_outputs=64, name=\"fc1\")\n",
    "\n",
    "    layer = new_activation_layer(layer, name=\"relu\")\n",
    "\n",
    "    layer = new_fc_layer(input=layer, num_inputs=64, keep_prob=keep_prob, num_outputs=8, name=\"fc2\")\n",
    "\n",
    "    layer = new_activation_layer(layer, name=\"relu\")\n",
    "    layer = new_fc_layer(input=layer, num_inputs=8, keep_prob=keep_prob, num_outputs=1, name=\"fc3\")\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def network_binary(x_input, keep_prob, args):\n",
    "    window_size = [128, 128]\n",
    "    stride_size = [64, 2]\n",
    "    num_filter = [32, 32]\n",
    "\n",
    "    layer = new_conv1d_layer(x_input, num_filter[0], window_size[0], stride_size[0], keep_prob, \"conv1\", 'relu', 'same')\n",
    "    layer = tf.keras.layers.BatchNormalization(name='bn1')(layer)\n",
    "\n",
    "    layer = new_conv1d_layer(layer, num_filter[1], window_size[1], stride_size[1], keep_prob, \"conv2\", 'relu', 'same')\n",
    "    layer = tf.keras.layers.BatchNormalization(name='bn2')(layer)\n",
    "    num_features = layer.get_shape()[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    layer = new_fc_layer(input=layer, num_inputs=num_features, keep_prob=keep_prob, num_outputs=64, name=\"fc1\")\n",
    "    layer = tf.keras.layers.BatchNormalization(name='bn3')(layer)\n",
    "\n",
    "\n",
    "    layer = new_fc_layer(input=layer, num_inputs=64, keep_prob=keep_prob,num_outputs=2, name=\"fc3\")\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def test_mdeep(x_test, args):\n",
    "    C = np.load('./c.npy')\n",
    "    hac_index = hac(C)\n",
    "    \n",
    "    x_test = x_test[:, hac_index]\n",
    "    model = tf.keras.models.load_model('./mdeep_trained.h5')\n",
    "   \n",
    "\n",
    "    outputs = model.predict([x_test, np.ones(len(x_test))])\n",
    "    y_predict_mdeep = np.where(outputs[:,0] > outputs[:, 1] , 0, 1)\n",
    "\n",
    "    return y_predict_mdeep\n",
    "\n",
    "def train_test(x_train, y_train, args):\n",
    "    n_classes = 2\n",
    "    num_epochs = args[\"num_epochs\"] #500\n",
    "    batch_size =  args[\"batch_size\"] #32\n",
    "    learning_rate = args[\"learning_rate\"] #0.0001\n",
    "    n_features = args[\"X_train\"].shape[1]\n",
    "    dropout_rate = args[\"dropout_rate\"] # 0.5\n",
    "\n",
    "    x = tf.keras.Input(shape=(n_features,))\n",
    "    y = tf.keras.Input(shape=(n_classes,))\n",
    "\n",
    "\n",
    "    keep_prob = tf.keras.Input(shape=(), dtype=tf.float32)\n",
    "\n",
    "    x_input = tf.reshape(x, [-1, n_features, 1])\n",
    "\n",
    "    layer = network_binary(x_input, keep_prob, args)  \n",
    "\n",
    "    model = tf.keras.Model(inputs=[x, keep_prob], outputs=layer)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    @tf.function\n",
    "    def train_step(model, optimizer, x, y, keep_prob):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model([x, keep_prob])\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x_tmp, y_tmp = shuffle(x_train, y_train)\n",
    "        total_batch = int(np.shape(x_train)[0] / batch_size)\n",
    "        \n",
    "        for i in range(total_batch - 1):\n",
    "            x_batch, y_true_batch = x_tmp[i * batch_size:i * batch_size + batch_size], \\\n",
    "                                    y_tmp[i * batch_size:i * batch_size + batch_size]\n",
    "            loss = train_step(model, optimizer, x_batch, y_true_batch, dropout_rate)\n",
    "\n",
    "        loss_val = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model([x_train, dropout_rate]), labels=y_train))\n",
    "        train_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(model([x_train, dropout_rate]), 1), tf.argmax(y_train, 1)), \"float\"))\n",
    "\n",
    "        print(\"Epoch {}, Loss: {:.4f}, Training accuracy: {:.4f}\".format(epoch, loss_val, train_accuracy))\n",
    "\n",
    "        if train_accuracy > 0.99:\n",
    "            break\n",
    "    model.save('./mdeep_trained.h5')\n",
    "    return test_mdeep(args[\"X_test\"].values, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_mdeep(args):\n",
    "    C = np.load('./c.npy')\n",
    "    print(\"Hierarchical clustering\")\n",
    "    hac_index = hac(C)\n",
    "    print(\"hac_index\")\n",
    "    print(hac_index)\n",
    "    print(\"Start training\")\n",
    "    X_train = args[\"X_train\"]\n",
    "    x_train = X_train.values\n",
    "    y_train = args[\"y_train\"]\n",
    "    y_train_values = y_train.values\n",
    "    y_tr = []\n",
    "    for l in y_train_values:\n",
    "        if l == 1:\n",
    "            y_tr.append([0, 1])\n",
    "        else:\n",
    "            y_tr.append([1, 0])\n",
    "    y_tr = np.array(y_tr, dtype=int)\n",
    "    x_train = x_train[:, hac_index]\n",
    "\n",
    "    return train_test(x_train, y_tr, args)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def model_predict_mdeep(xtest):\n",
    "    model = tf.keras.models.load_model('./mdeep_trained.h5')\n",
    "    outputs = model.predict([xtest, np.ones(len(xtest))])\n",
    "    mdeep_y = np.where((outputs[:,0]) >= (outputs[:, 1]) , 0, 1)\n",
    "    return mdeep_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
